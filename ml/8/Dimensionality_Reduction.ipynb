{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4d8eef",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0f3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"dim_reduction\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261c22d5",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction \n",
    "\n",
    "\"Many Machine Learning problems involve thousands or even millions of features for  \n",
    "each training instance. Not only do all these features make training extremely slow,  \n",
    "but they can also make it much harder to find a good solution, as we will see. This  \n",
    "problem is often referred to as the curse of dimensionality.\"\n",
    "\n",
    "WARNING: \"Reducing dimensionality does cause some information loss (just  \n",
    "like compressing an image to JPEG can degrade its quality), so  \n",
    "even though it will speed up training, it may make your system  \n",
    "perform slightly worse. It also makes your pipelines a bit more  \n",
    "complex and thus harder to maintain. So, if training is too slow,  \n",
    "you should first try to train your system with the original data  \n",
    "before considering using dimensionality reduction. In some cases,  \n",
    "reducing the dimensionality of the training data may filter out  \n",
    "some noise and unnecessary details and thus result in higher performance,  \n",
    "but in general it won’t; it will just speed up training.\"\n",
    "\n",
    "\"Apart from speeding up training, dimensionality reduction is also extremely useful  \n",
    "for data visualization (or DataViz). Reducing the number of dimensions down to two  \n",
    "(or three) makes it possible to plot a condensed view of a high-dimensional training  \n",
    "set on a graph and often gain some important insights by visually detecting patterns,  \n",
    "such as clusters. Moreover, DataViz is essential to communicate your conclusions to  \n",
    "people who are not data scientists—in particular, decision makers who will use your  \n",
    "results.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fc1e14",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality \n",
    "\n",
    "  \n",
    "\"The more dimensions the training set has, the greater the risk of overfitting it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ca9c42",
   "metadata": {},
   "source": [
    "# Main Approaches for Dimensionality Reduction\n",
    "\n",
    "## Projection\n",
    "\n",
    "\"In most real-world problems, training instances are not spread out uniformly across  \n",
    "all dimensions. Many features are almost constant, while others are highly correlated  \n",
    "(as discussed earlier for MNIST). As a result, all training instances lie within (or close  \n",
    "to) a much lower-dimensional subspace of the high-dimensional space. This sounds  \n",
    "very abstract, so let’s look at an example. In Figure 8-2 you can see a 3D dataset represented  \n",
    "by circles.\"\n",
    "\n",
    "![title](images/proj_1.png)\n",
    "\n",
    "\"Notice that all training instances lie close to a plane: this is a lower-dimensional (2D)  \n",
    "subspace of the high-dimensional (3D) space. If we project every training instance  \n",
    "perpendicularly onto this subspace (as represented by the short lines connecting the  \n",
    "instances to the plane), we get the new 2D dataset shown in Figure 8-3. Ta-da! We  \n",
    "have just reduced the dataset’s dimensionality from 3D to 2D. Note that the axes correspond  \n",
    "to new features z1 and z2 (the coordinates of the projections on the plane).\"  \n",
    "\n",
    "![title](images/proj_2.png)\n",
    "\n",
    "\"However, projection is not always the best approach to dimensionality reduction. In  \n",
    "many cases the subspace may twist and turn, such as in the famous Swiss roll toy dataset  \n",
    "represented in Figure 8-4.\"\n",
    "\n",
    "![title](images/proj_3.png)\n",
    "\n",
    "\"Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of  \n",
    "the Swiss roll together, as shown on the left side of Figure 8-5. What you really want is  \n",
    "to unroll the Swiss roll to obtain the 2D dataset on the right side of Figure 8-5.\"\n",
    "\n",
    "![title](images/proj_4.png)\n",
    "\n",
    "\n",
    "\n",
    "## Manifold \n",
    "\n",
    "\"The Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D  \n",
    "shape that can be bent and twisted in a higher-dimensional space. More generally, a  \n",
    "d-dimensional manifold is a part of an n-dimensional space (where d < n) that locally  \n",
    "resembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it  \n",
    "locally resembles a 2D plane, but it is rolled in the third dimension.\"\n",
    "\n",
    "\"Many dimensionality reduction algorithms work by modeling the manifold on which  \n",
    "the training instances lie; this is called Manifold Learning. It relies on the manifold  \n",
    "assumption, also called the manifold hypothesis, which holds that most real-world  \n",
    "high-dimensional datasets lie close to a much lower-dimensional manifold. This  \n",
    "assumption is very often empirically observed.\"\n",
    "\n",
    "\"The manifold assumption is often accompanied by another implicit assumption: that  \n",
    "the task at hand (e.g., classification or regression) will be simpler if expressed in the  \n",
    "lower-dimensional space of the manifold. For example, in the top row of Figure 8-6  \n",
    "the Swiss roll is split into two classes: in the 3D space (on the left), the decision  \n",
    "boundary would be fairly complex, but in the 2D unrolled manifold space (on the  \n",
    "right), the decision boundary is a straight line.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4588bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c20a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_learning] *",
   "language": "python",
   "name": "conda-env-ml_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
