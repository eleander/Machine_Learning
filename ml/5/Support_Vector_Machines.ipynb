{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97fc5326",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "173fba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"svm\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f6da48",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "\"A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning\n",
    "model, capable of performing linear or nonlinear classification, regression, and even\n",
    "outlier detection. It is one of the most popular models in Machine Learning, and anyone\n",
    "interested in Machine Learning should have it in their toolbox. SVMs are particularly\n",
    "well suited for classification of complex but small- or medium-sized datasets.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3fbafc",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "\n",
    "\"The two classes can clearly be separated easily with a straight line (they are linearly separable).  \n",
    "The left plot shows the decision boundaries of three possible linear classifiers. The  \n",
    "model whose decision boundary is represented by the dashed line is so bad that it  \n",
    "does not even separate the classes properly. The other two models work perfectly on  \n",
    "this training set, but their decision boundaries come so close to the instances that  \n",
    "these models will probably not perform as well on new instances. In contrast, the  \n",
    "solid line in the plot on the right represents the decision boundary of an SVM classifier;  \n",
    "this line not only separates the two classes but also stays as far away from the  \n",
    "closest training instances as possible. You can think of an SVM classifier as fitting the\"  \n",
    "widest possible street (represented by the parallel dashed lines) between the classes.  \n",
    "This is called large margin classification.\n",
    "\n",
    "![title](images/svm_1.png)\n",
    "\n",
    "\"SVMs are sensitive to the feature scales, as you can see in  \n",
    "Figure 5-2: on the left plot, the vertical scale is much larger than the  \n",
    "horizontal scale, so the widest possible street is close to horizontal.  \n",
    "After feature scaling (e.g., using Scikit-Learn’s StandardScaler),  \n",
    "the decision boundary looks much better (on the right plot).\"\n",
    "\n",
    "![title](images/svm_2.png)\n",
    "\n",
    "## Soft Margin Classification\n",
    "\n",
    "\"If we strictly impose that all instances be off the street and on the right side, this is  \n",
    "called hard margin classification. There are two main issues with hard margin classification.  \n",
    "First, it only works if the data is linearly separable, and second it is quite sensitive  \n",
    "to outliers\"\n",
    "\n",
    "![title](images/svm_3.png)\n",
    "\n",
    "\"To avoid these issues it is preferable to use a more flexible model. The objective is to  \n",
    "find a good balance between keeping the street as large as possible and limiting the  \n",
    "margin violations (i.e., instances that end up in the middle of the street or even on the  \n",
    "wrong side). This is called soft margin classification.\"\n",
    "\n",
    "\"In Scikit-Learn’s SVM classes, you can control this balance using the C hyperparameter:  \n",
    "a smaller C value leads to a wider street but more margin violations. Figure 5-4  \n",
    "shows the decision boundaries and margins of two soft margin SVM classifiers on a  \n",
    "nonlinearly separable dataset. On the left, using a low C value the margin is quite   \n",
    "large, but many instances end up on the street. On the right, using a high C value the  \n",
    "classifier makes fewer margin violations but ends up with a smaller margin. However,  \n",
    "it seems likely that the first classifier will generalize better: in fact even on this training  \n",
    "set it makes fewer prediction errors, since most of the margin violations are  \n",
    "actually on the correct side of the decision boundary.\"\n",
    "\n",
    "![title](images/svm_4.png)\n",
    "\n",
    "TIP: \"If your SVM model is overfitting, you can try regularizing it by  \n",
    "reducing C.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de4306d",
   "metadata": {},
   "source": [
    "\"The following Scikit-Learn code loads the iris dataset, scales the features, and then  \n",
    "trains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss  \n",
    "function, described shortly) to detect Iris-Virginica flowers. The resulting model is  \n",
    "represented on the left of Figure 5-4.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a90b85e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge', random_state=42))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n",
    "    ])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63c3e676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa835ae9",
   "metadata": {},
   "source": [
    "NOTE: \"Unlike Logistic Regression classifiers, SVM classifiers do not output  \n",
    "probabilities for each class.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18051dfc",
   "metadata": {},
   "source": [
    "\"Alternatively, you could use the SVC class, using SVC(kernel=\"linear\", C=1), but it  \n",
    "is much slower, especially with large training sets, so it is not recommended. Another  \n",
    "option is to use the SGDClassifier class, with SGDClassifier(loss=\"hinge\",  \n",
    "alpha=1/(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to  \n",
    "train a linear SVM classifier. It does not converge as fast as the LinearSVC class, but it  \n",
    "can be useful to handle huge datasets that do not fit in memory (out-of-core training),  \n",
    "or to handle online classification tasks.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaab9a81",
   "metadata": {},
   "source": [
    "TIP: \"The LinearSVC class regularizes the bias term, so you should center  \n",
    "the training set first by subtracting its mean. This is automatic if  \n",
    "you scale the data using the StandardScaler. Moreover, make sure  \n",
    "you set the loss hyperparameter to \"hinge\", as it is not the default  \n",
    "value. Finally, for better performance you should set the dual  \n",
    "hyperparameter to False, unless there are more features than  \n",
    "training instances (we will discuss duality later in the chapter).\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml_learning] *",
   "language": "python",
   "name": "conda-env-ml_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}